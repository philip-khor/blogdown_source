
# A critique of "Malaysian Election data dimension importance analysis via RandomForest by Jamie K" 
The research questions were: 
1. Does ethnicity matter more than age? Or does age matter more than ethnicity?
2. Which ethnic group matters the most in deciding a constituency’s election outcome?
3. Which age group matters the most in deciding a constituency’s election outcome?


```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib notebook
color_cycle = 'bgrcmyk'
```

    /Users/philip/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
      from numpy.core.umath_tests import inner1d



```python
ge14 = pd.read_excel("GE14_Age-Ethnicity-bySeats.xlsx", sheet_name = "age and ethnicity")
ge14.describe().T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>kodparlimen</th>
      <td>222.0</td>
      <td>111.500000</td>
      <td>64.230055</td>
      <td>1.00</td>
      <td>56.2500</td>
      <td>111.500</td>
      <td>166.7500</td>
      <td>222.00</td>
    </tr>
    <tr>
      <th>21 - 30 (%)</th>
      <td>222.0</td>
      <td>17.338874</td>
      <td>4.057915</td>
      <td>5.62</td>
      <td>14.9725</td>
      <td>17.475</td>
      <td>19.2700</td>
      <td>30.89</td>
    </tr>
    <tr>
      <th>31 - 40 (%)</th>
      <td>222.0</td>
      <td>23.829369</td>
      <td>3.118346</td>
      <td>11.99</td>
      <td>22.1675</td>
      <td>23.760</td>
      <td>25.7825</td>
      <td>39.50</td>
    </tr>
    <tr>
      <th>41 - 50 (%)</th>
      <td>222.0</td>
      <td>20.243784</td>
      <td>1.866487</td>
      <td>14.98</td>
      <td>18.9350</td>
      <td>20.140</td>
      <td>21.5500</td>
      <td>25.63</td>
    </tr>
    <tr>
      <th>51 - 60 (%)</th>
      <td>222.0</td>
      <td>18.179550</td>
      <td>2.191625</td>
      <td>11.97</td>
      <td>16.9425</td>
      <td>18.295</td>
      <td>19.3675</td>
      <td>24.26</td>
    </tr>
    <tr>
      <th>61 - 70 (%)</th>
      <td>222.0</td>
      <td>12.264820</td>
      <td>2.495656</td>
      <td>6.88</td>
      <td>10.4425</td>
      <td>12.195</td>
      <td>13.5700</td>
      <td>22.21</td>
    </tr>
    <tr>
      <th>71 - 80 (%)</th>
      <td>222.0</td>
      <td>5.716396</td>
      <td>1.582075</td>
      <td>0.58</td>
      <td>4.5575</td>
      <td>5.670</td>
      <td>6.6500</td>
      <td>11.41</td>
    </tr>
    <tr>
      <th>81 - 90 (%)</th>
      <td>222.0</td>
      <td>1.923468</td>
      <td>0.727520</td>
      <td>0.11</td>
      <td>1.4125</td>
      <td>1.830</td>
      <td>2.3625</td>
      <td>4.40</td>
    </tr>
    <tr>
      <th>Above 90 (%)</th>
      <td>222.0</td>
      <td>0.503333</td>
      <td>0.377438</td>
      <td>0.00</td>
      <td>0.2725</td>
      <td>0.395</td>
      <td>0.5900</td>
      <td>2.33</td>
    </tr>
    <tr>
      <th>Melayu (%)</th>
      <td>222.0</td>
      <td>51.867658</td>
      <td>30.856729</td>
      <td>0.63</td>
      <td>22.9125</td>
      <td>59.120</td>
      <td>78.0025</td>
      <td>99.41</td>
    </tr>
    <tr>
      <th>Cina (%)</th>
      <td>222.0</td>
      <td>24.729099</td>
      <td>21.508197</td>
      <td>0.20</td>
      <td>7.8075</td>
      <td>18.445</td>
      <td>36.2550</td>
      <td>88.74</td>
    </tr>
    <tr>
      <th>India (%)</th>
      <td>222.0</td>
      <td>5.706622</td>
      <td>6.341847</td>
      <td>0.01</td>
      <td>0.1925</td>
      <td>3.405</td>
      <td>10.2475</td>
      <td>27.69</td>
    </tr>
    <tr>
      <th>Bumiputera Sabah (%)</th>
      <td>222.0</td>
      <td>7.795045</td>
      <td>22.518421</td>
      <td>0.02</td>
      <td>0.0600</td>
      <td>0.155</td>
      <td>0.3200</td>
      <td>94.83</td>
    </tr>
    <tr>
      <th>Bumiputera Sarawak (%)</th>
      <td>222.0</td>
      <td>8.082387</td>
      <td>22.265801</td>
      <td>0.02</td>
      <td>0.0500</td>
      <td>0.145</td>
      <td>0.6300</td>
      <td>96.74</td>
    </tr>
    <tr>
      <th>Orang Asli (%)</th>
      <td>222.0</td>
      <td>0.670811</td>
      <td>2.243143</td>
      <td>0.00</td>
      <td>0.0000</td>
      <td>0.020</td>
      <td>0.1775</td>
      <td>21.56</td>
    </tr>
    <tr>
      <th>Lain-Lain (%)</th>
      <td>222.0</td>
      <td>1.147477</td>
      <td>2.386193</td>
      <td>0.07</td>
      <td>0.2400</td>
      <td>0.360</td>
      <td>0.8175</td>
      <td>20.67</td>
    </tr>
  </tbody>
</table>
</div>




```python
agevars = ['21 - 30 (%)', '31 - 40 (%)',
       '41 - 50 (%)', '51 - 60 (%)', '61 - 70 (%)', '71 - 80 (%)',
       '81 - 90 (%)', 'Above 90 (%)']
racevars = ['Melayu (%)', 'Cina (%)', 'India (%)',
       'Bumiputera Sabah (%)', 'Bumiputera Sarawak (%)', 'Orang Asli (%)',
       'Lain-Lain (%)']
features = agevars + racevars
```


```python
ge14 = ge14.rename(columns = {'GE 2018 COALITION WINNER':'target'})
```

# Who won? 

Thought I'd just do a quick waffle chart using the incredible `pywaffle` package. 


```python
from pywaffle import Waffle
data = ge14.target.value_counts()
fig = plt.figure(
    FigureClass=Waffle, 
    rows=10, 
    values=data, 
    colors=("red", "#232066", "green", "lightblue", 'grey', 'cyan'),
    title={'label': '14th Malaysian Parliament composition', 'loc': 'left'},
    labels=["{0}".format(k) for k, v in data.items()],
    icons = 'user',
    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)}
)
fig.gca().set_facecolor('#EEEEEE')
fig.set_facecolor('#EEEEEE')
fig.set_tight_layout(False)
plt.show()
```


![png](output_6_0.png)


# Overall distribution of demographics across constituencies

With the data we have, there are two stories to be told: 
* **Voting behaviour by ethnicity**: 
    * **Gagasan Sejahtera** does well in constituencies with high Malay concentration, low Chinese and Indian concentration. 
    * The distributions for Bumiputera Sarawak/Sabah suggest we should analyse by Peninsula/Borneo. 
    * For BN, the distribution of wins by ethnicity are more tightly distributed around low Indian and Chinese population constituencies. Both BN and PH have wins across a range of Malay-minority and Malay-majority constituencies.  
* **Voting behaviour by age**: 
    * BN and PH seem to have an edge in 'older' constituencies.
    * PAS seems to win in younger constituencies (higher pct 21-30, 31-40)
    * PAS wins are very uniform in terms of age and ethnicity demographics! 


```python
fg, ax = plt.subplots(nrows =4, figsize = (6,20))
ge14[ge14.target =='BN'][features].boxplot(vert = False, ax = ax[0])
ax[0].set_title("BN")
ge14[ge14.target=='PH'][features].boxplot(vert = False, ax = ax[1])
ax[1].set_title("PH")
ge14[ge14.target=='Gagasan Sejahtera'][features].boxplot(vert = False, ax = ax[2])
ax[2].set_title("Gagasan Sejahtera")
ge14[~ge14.target.isin(['BN', 'PH', 'Gagasan Sejahtera'])][features].boxplot(vert = False, ax = ax[3])
ax[3].set_title("Others")

plt.tight_layout()
```


![png](output_8_0.png)


# In what sort of seats do these parties win? 


## Ethnic diversity of constituencies
Using `scipy`'s default entropy function to compute a diversity index for each constituency, I try to determine which parties win in ethnically 'diverse' and 'less-diverse' constituencies. This kind of confirms what we already know from the boxplots above.  


```python
from scipy.stats import entropy
entropy_const = np.array([entropy(ge14[racevars].iloc[i,:].tolist()) for i in range(len(ge14))])
```


```python
np.max(entropy_const)
```




    1.374261954422862




```python
ge14.Negeri.unique()
```




    array(['PERLIS', 'KEDAH', 'KELANTAN', 'TERENGGANU', 'PULAU PINANG',
           'PERAK', 'PAHANG', 'SELANGOR', 'W.P KUALA LUMPUR', 'W.P PUTRAJAYA',
           'NEGERI SEMBILAN', 'MELAKA', 'JOHOR', 'W.P LABUAN', 'SABAH',
           'SARAWAK'], dtype=object)




```python
equiv_entropy = np.zeros(len(entropy_const))
```


```python
# use effective number of species instead https://medium.com/@sam.weinger/how-diverse-are-names-in-america-f74b07e031bd
effective_number = np.exp(entropy_const)
```


```python
np.mean(effective_number[ge14.target=='BN']), np.mean(effective_number[ge14.target=='PH']), np.mean(effective_number[ge14.target=='Gagasan Sejahtera'])
```




    (2.023986123800037, 2.4850578347860295, 1.2839901761045327)




```python
fg, ax = plt.subplots(nrows = 3, figsize = (5,5))
sns.boxplot(effective_number[ge14.target=='BN'], ax = ax[0])
ax[0].set_title("BN")

sns.boxplot(effective_number[ge14.target=='PH'], ax = ax[1], color = 'r')
ax[1].set_title("PH")
sns.boxplot(effective_number[ge14.target=='Gagasan Sejahtera'], ax = ax[2], color = 'g')
ax[2].set_title("Gagasan Sejahtera")
# plt.xlim(0,)
plt.tight_layout()
print('\nPH wins in diverse and non-diverse constituencies;\nPAS and BN win in non-diverse constituencies')
```

    BN wins across diverse and non-diverse constituencies; 
    PH wins in diverse constituencies;
    PAS wins in non-diverse constituencies



![png](output_17_1.png)


# Peninsula-Borneo divide? 
Because there are different parties in Peninsula and Borneo, I calculate statistics separately for Peninsula and Borneo respectively. I use the demographic with the highest share in each constituency. 

## Peninsula 

PH wins all Chinese-majority seats in the Peninsula, and most Malay-majority seats in the Peninsula. 


```python
peninsula = [i not in ['SABAH', 'SARAWAK', 'W.P LABUAN'] for i in ge14.Negeri]
```


```python
ethnicity = ge14[racevars].idxmax(axis = 1)
```


```python
age = ge14[agevars].idxmax(axis = 1)
```


```python
pd.crosstab(ge14[peninsula].target, ethnicity)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>Cina (%)</th>
      <th>Melayu (%)</th>
    </tr>
    <tr>
      <th>target</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>BN</th>
      <td>0</td>
      <td>49</td>
    </tr>
    <tr>
      <th>Gagasan Sejahtera</th>
      <td>0</td>
      <td>18</td>
    </tr>
    <tr>
      <th>PH</th>
      <td>33</td>
      <td>65</td>
    </tr>
  </tbody>
</table>
</div>



All Peninsula parties with seats have the most seats in constituencies with the most people between 31-40 years of age. 


```python
pd.crosstab(ge14[peninsula].target, age)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>21 - 30 (%)</th>
      <th>31 - 40 (%)</th>
      <th>41 - 50 (%)</th>
      <th>51 - 60 (%)</th>
    </tr>
    <tr>
      <th>target</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>BN</th>
      <td>2</td>
      <td>47</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Gagasan Sejahtera</th>
      <td>3</td>
      <td>15</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>PH</th>
      <td>0</td>
      <td>77</td>
      <td>11</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>



## Borneo
BN is strongest in indigenous seats whereas PH is strongest in Chinese seats. PH made up for its weakness in indigenous seats with its partnership with WARISAN. Both BN and PH are strongest in age-31-40-majority seats. 


```python
pd.crosstab(ge14[np.invert(peninsula)].target, ethnicity)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>Bumiputera Sabah (%)</th>
      <th>Bumiputera Sarawak (%)</th>
      <th>Cina (%)</th>
      <th>Melayu (%)</th>
    </tr>
    <tr>
      <th>target</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>BEBAS</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>BN</th>
      <td>10</td>
      <td>14</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>PH</th>
      <td>3</td>
      <td>4</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <th>STAR</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>WARISAN</th>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.crosstab(ge14[np.invert(peninsula)].target, age)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>31 - 40 (%)</th>
      <th>41 - 50 (%)</th>
      <th>51 - 60 (%)</th>
    </tr>
    <tr>
      <th>target</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>BEBAS</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>BN</th>
      <td>17</td>
      <td>11</td>
      <td>2</td>
    </tr>
    <tr>
      <th>PH</th>
      <td>8</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>STAR</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>WARISAN</th>
      <td>8</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



# Interpreting random forests

## Fitting the model

While looking at the code in the DataTarik article, I wasn't too sure what to think of fitting random forests over a sample size of 222. Well, 222 *is* the population, but with a test set sized at 10% of the dataset (20!?!?!?), it's quite possible that the accuracy of 91% was obtained by chance. Changing the random state gave me a much lower accuracy (50+%), as expected. To be conservative I set a `test_size` of .4, but I don't think even that's OK either ... 

It's quite likely that the distribution of constituencies in the test and training set is not alike. At the risk of oversimplifying, I stratify the `train_test_split` by whether a seat is in the Peninsula or not. 

To be honest, I'm not a fan of this approach. There are 6 classes, and one class with just one observation. I'd have grouped the classes, e.g. since PH had an electoral alliance with WARISAN, it seems to make sense to group the two together -  but it doesn't seem like the original author did. So I'm sticking to how he prepared the data. 


```python
ge14 = ge14.set_index('namaparlimen')
#Split the data into train & test sets (0.90 & 0.10 size resepectively)
#Fixed variable for random state to enable replication of results
X_train, X_test, Y_train, Y_test = train_test_split(ge14[features], 
                                                    ge14.target, 
                                                    test_size = 0.4, 
                                                    stratify = peninsula,
                                                    random_state = 42)

#Initiate and train random forest model. Custom parameters:
#n_estimators: default is 10 but we are using 100 to reduce overfitting
#Fixed variable for random state to enable replication of results
clf = RandomForestClassifier(n_estimators = 100, random_state = 3, oob_score = True)
clf.fit(X_train, Y_train)

print ("Accuracy of random forest in predicting electoral outcomes based on age group composition.")
print ("-----------------")
print ("Hygiene check: Accuracy of prediction on TRAIN data split (should be 100% accurate): {}".format(clf.score(X_train, Y_train)))
print ("Accuracy of prediction on TEST data split: {}".format(clf.score(X_test, Y_test)))
print ("-----------------")
```

    Accuracy of random forest in predicting electoral outcomes based on age group composition.
    -----------------
    Hygiene check: Accuracy of prediction on TRAIN data split (should be 100% accurate): 1.0
    Accuracy of prediction on TEST data split: 0.7640449438202247
    -----------------


### Interpreting Random Forest Results

76% is a lot worse than 91%; but the 91% model might not be generalisable. 
I take a look at the confusion matrix. 


```python
from sklearn.metrics import confusion_matrix 
rf_pred = clf.predict(X_test)
labels = ['BEBAS', 'BN', 'Gagasan Sejahtera', 'PH', 'WARISAN']
cm = confusion_matrix(Y_test, rf_pred, labels = labels)
print("Rows represent predicted class, columns represent actual class")
pd.DataFrame(cm, index = labels, columns=labels)
```

    Rows represent predicted class, columns represent actual class





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>BEBAS</th>
      <th>BN</th>
      <th>Gagasan Sejahtera</th>
      <th>PH</th>
      <th>WARISAN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>BEBAS</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>BN</th>
      <td>1</td>
      <td>21</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Gagasan Sejahtera</th>
      <td>0</td>
      <td>2</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>PH</th>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>40</td>
      <td>1</td>
    </tr>
    <tr>
      <th>WARISAN</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



Here are some of the seats that were correctly predicted: 


```python
test_set = list(zip(X_test.index, Y_test, rf_pred))
test_df = pd.DataFrame(test_set, columns = ['seat', 'actual', 'prediction'])
```


```python
test_df[test_df.actual == test_df.prediction].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>seat</th>
      <th>actual</th>
      <th>prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>KUBANG KERIAN</td>
      <td>Gagasan Sejahtera</td>
      <td>Gagasan Sejahtera</td>
    </tr>
    <tr>
      <th>2</th>
      <td>BATANG LUPAR</td>
      <td>BN</td>
      <td>BN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>BUKIT BENDERA</td>
      <td>PH</td>
      <td>PH</td>
    </tr>
    <tr>
      <th>4</th>
      <td>RASAH</td>
      <td>PH</td>
      <td>PH</td>
    </tr>
    <tr>
      <th>5</th>
      <td>KLUANG</td>
      <td>PH</td>
      <td>PH</td>
    </tr>
  </tbody>
</table>
</div>



And the seats that were incorrectly predicted: 


```python
test_df[test_df.actual != test_df.prediction].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>seat</th>
      <th>actual</th>
      <th>prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SEMPORNA</td>
      <td>WARISAN</td>
      <td>BN</td>
    </tr>
    <tr>
      <th>12</th>
      <td>PUTATAN</td>
      <td>PH</td>
      <td>WARISAN</td>
    </tr>
    <tr>
      <th>17</th>
      <td>PADANG TERAP</td>
      <td>BN</td>
      <td>Gagasan Sejahtera</td>
    </tr>
    <tr>
      <th>19</th>
      <td>BESUT</td>
      <td>BN</td>
      <td>Gagasan Sejahtera</td>
    </tr>
    <tr>
      <th>24</th>
      <td>SARATOK</td>
      <td>PH</td>
      <td>BN</td>
    </tr>
  </tbody>
</table>
</div>



### According to the algorithm, which is the most important age and ethnic group in deciding the election winner?

The author claims that `scikit-learn`'s `feature_importances_` attribute is
> An unbiased method of learning the 'most influential age group' and 'most influential ethnicity group' in deciding Y2018's election results.

But is it really? [This site](http://explained.ai/rf-importance/) discusses how the default feature importances provided by scikit-learn are biased. But first, I replicate the results with my new model to see if the results change. There are slight changes in the rank, but nothing too drastic. 

The following results are produced using the author's original code. 


```python
%matplotlib inline 

importance = pd.Series(clf.feature_importances_, index=X_train.columns)
importance.index = importance.index.map(lambda x: str(x)[:-4])

f_importance = importance.to_frame()
f_importance.rename(columns = {0 : "Importance Weigh"}, inplace = True)
f_importance.sort_values("Importance Weigh",ascending = True, inplace = True)
f_importance.loc[:,"Importance Weigh"] = f_importance["Importance Weigh"]*100

f_importance.plot(kind='barh', title = "Importance of age and ethnicity groups ranked", legend = False, color = color_cycle)
plt.tight_layout()
```


![png](output_42_0.png)



```python
f_importance
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Importance Weigh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>81 - 90</th>
      <td>3.290618</td>
    </tr>
    <tr>
      <th>Above 90</th>
      <td>3.622834</td>
    </tr>
    <tr>
      <th>71 - 80</th>
      <td>3.950714</td>
    </tr>
    <tr>
      <th>61 - 70</th>
      <td>4.006856</td>
    </tr>
    <tr>
      <th>Orang Asli</th>
      <td>4.203923</td>
    </tr>
    <tr>
      <th>41 - 50</th>
      <td>4.334116</td>
    </tr>
    <tr>
      <th>21 - 30</th>
      <td>5.202408</td>
    </tr>
    <tr>
      <th>31 - 40</th>
      <td>5.616519</td>
    </tr>
    <tr>
      <th>Bumiputera Sabah</th>
      <td>6.084498</td>
    </tr>
    <tr>
      <th>Lain-Lain</th>
      <td>6.411824</td>
    </tr>
    <tr>
      <th>Bumiputera Sarawak</th>
      <td>6.573845</td>
    </tr>
    <tr>
      <th>51 - 60</th>
      <td>7.265262</td>
    </tr>
    <tr>
      <th>Melayu</th>
      <td>11.254242</td>
    </tr>
    <tr>
      <th>India</th>
      <td>12.549527</td>
    </tr>
    <tr>
      <th>Cina</th>
      <td>15.632814</td>
    </tr>
  </tbody>
</table>
</div>




```python
age_importance = f_importance.loc[('81 - 90','41 - 50','61 - 70','Above > 90','71 - 80','31 - 40','21 - 30','51 - 60'),:].copy()
age_importance.sort_values(by = 'Importance Weigh',ascending = True, inplace = True)

ethnicity_importance = f_importance.loc[('Cina','India','Melayu','Bumiputera Sabah','Bumiputera Sarawak','Lain-Lain','Orang Asli'),:].copy()
ethnicity_importance.sort_values(by = 'Importance Weigh',ascending = True, inplace = True)

fig, axes = plt.subplots(ncols=2, figsize=(12, 8))
ethnicity_importance.plot(ax = axes[0], kind='barh', title = "Ethnicity group ranked", legend = False, color = color_cycle)
age_importance.plot(ax = axes[1],kind='barh', title = "Age group ranked", legend = False, color = color_cycle)
plt.tight_layout()
#plt.savefig("Age and Ethnicity Ranked_Separately.png")
#plt.show()
```

    /Users/philip/miniconda3/lib/python3.6/site-packages/pandas/core/indexing.py:870: FutureWarning: 
    Passing list-likes to .loc or [] with any missing label will raise
    KeyError in the future, you can use .reindex() as an alternative.
    
    See the documentation here:
    https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike
      return self._getitem_lowerdim(tup)



![png](output_44_1.png)


## Critique 
Sample size issues aside, by referring to the explained.ai article, it seems there are several issues with this approach to feature importance. 
### 1. Default feature importances in scikit-learn is biased. 
The mean decrease in impurity version of feature importances implemented in scikit-learn is a problem if 
* variables vary in their scale of measurement 
* variables vary in number of categories

If all variables are continuous, using the default is fine. 
In any case, we compute an alternative approach to feature importances: permutation importance - 
* calculate a baseline score 
* permute a column of values, calculate score 
* the drop in the second score from the baseline due to the permutation is the importance score. 




```python
!pip install rfpimp
```

    Collecting rfpimp
      Downloading https://files.pythonhosted.org/packages/a5/db/c07a89d49fb22da83d1485dc76adebca39c5d760342750d58853315bd81a/rfpimp-1.2.tar.gz
    Requirement already satisfied: numpy in /Users/philip/miniconda3/lib/python3.6/site-packages (from rfpimp) (1.15.0)
    Requirement already satisfied: pandas in /Users/philip/miniconda3/lib/python3.6/site-packages (from rfpimp) (0.23.4)
    Collecting sklearn (from rfpimp)
      Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz
    Requirement already satisfied: matplotlib in /Users/philip/miniconda3/lib/python3.6/site-packages (from rfpimp) (2.2.3)
    Requirement already satisfied: python-dateutil>=2.5.0 in /Users/philip/miniconda3/lib/python3.6/site-packages (from pandas->rfpimp) (2.7.3)
    Requirement already satisfied: pytz>=2011k in /Users/philip/miniconda3/lib/python3.6/site-packages (from pandas->rfpimp) (2018.5)
    Requirement already satisfied: scikit-learn in /Users/philip/miniconda3/lib/python3.6/site-packages (from sklearn->rfpimp) (0.19.1)
    Requirement already satisfied: cycler>=0.10 in /Users/philip/miniconda3/lib/python3.6/site-packages (from matplotlib->rfpimp) (0.10.0)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/philip/miniconda3/lib/python3.6/site-packages (from matplotlib->rfpimp) (2.2.0)
    Requirement already satisfied: six>=1.10 in /Users/philip/miniconda3/lib/python3.6/site-packages (from matplotlib->rfpimp) (1.11.0)
    Requirement already satisfied: kiwisolver>=1.0.1 in /Users/philip/miniconda3/lib/python3.6/site-packages (from matplotlib->rfpimp) (1.0.1)
    Requirement already satisfied: setuptools in /Users/philip/miniconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->rfpimp) (40.0.0)
    Building wheels for collected packages: rfpimp, sklearn
      Running setup.py bdist_wheel for rfpimp ... [?25ldone
    [?25h  Stored in directory: /Users/philip/Library/Caches/pip/wheels/ce/d4/33/06727dd13f64581b317c468865e75d224e90ad0e690dc7e36a
      Running setup.py bdist_wheel for sklearn ... [?25ldone
    [?25h  Stored in directory: /Users/philip/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074
    Successfully built rfpimp sklearn
    [31mmkl-random 1.0.1 requires cython, which is not installed.[0m
    [31mmkl-fft 1.0.4 requires cython, which is not installed.[0m
    Installing collected packages: sklearn, rfpimp
    Successfully installed rfpimp-1.2 sklearn-0.0
    [33mYou are using pip version 10.0.1, however version 18.0 is available.
    You should consider upgrading via the 'pip install --upgrade pip' command.[0m



```python
from rfpimp import permutation_importances, oob_classifier_accuracy
permutation_importances(clf, X_train, Y_train, oob_classifier_accuracy)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Importance</th>
    </tr>
    <tr>
      <th>Feature</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Cina (%)</th>
      <td>0.165414</td>
    </tr>
    <tr>
      <th>India (%)</th>
      <td>0.075188</td>
    </tr>
    <tr>
      <th>Melayu (%)</th>
      <td>0.060150</td>
    </tr>
    <tr>
      <th>Orang Asli (%)</th>
      <td>0.060150</td>
    </tr>
    <tr>
      <th>Lain-Lain (%)</th>
      <td>0.030075</td>
    </tr>
    <tr>
      <th>41 - 50 (%)</th>
      <td>0.022556</td>
    </tr>
    <tr>
      <th>71 - 80 (%)</th>
      <td>0.022556</td>
    </tr>
    <tr>
      <th>21 - 30 (%)</th>
      <td>0.015038</td>
    </tr>
    <tr>
      <th>31 - 40 (%)</th>
      <td>0.015038</td>
    </tr>
    <tr>
      <th>51 - 60 (%)</th>
      <td>0.015038</td>
    </tr>
    <tr>
      <th>61 - 70 (%)</th>
      <td>0.015038</td>
    </tr>
    <tr>
      <th>Bumiputera Sabah (%)</th>
      <td>0.007519</td>
    </tr>
    <tr>
      <th>Bumiputera Sarawak (%)</th>
      <td>0.007519</td>
    </tr>
    <tr>
      <th>81 - 90 (%)</th>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Above 90 (%)</th>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



Using permutation bumps all the ethnicities to the top except for Bumiputera Sabah and Sarawak. 

## 2. Near-perfect multicollinearity 

As discussed [here](http://explained.ai/rf-importance/index.html#corr_collinear), collinear features can produce incorrect feature importance scores. Let's diagnose the multicollinearity in our variables. 
* Because the age and ethnicity variables add up to 100%, we can simply sum up the percentages and see how close they are to 100%. 
* It turns out most constituencies add up to 100%. Our feature importance scores are likely to be incorrect. 
* If all variables add up to 100%, there is perfect collinearity. Take the example of two percentages, A and not A. If A = .2, not A is necessarily .8. 
* The heatmap below indicates strong multicollinearity. 


```python
corr = ge14.iloc[:,3:-2].corr()
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(corr, mask = mask)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a15be3e48>




![png](output_50_1.png)



```python
fg, ax = plt.subplots(ncols = 2, figsize = (10,5)) 

ge14.iloc[:,2:10].sum(axis = 1).plot.box(ax = ax[0])
ax[0].ticklabel_format(useOffset=False, axis = 'y')
ax[0].set_title('Distribution of sum of age variables')

ge14.iloc[:,10:-2].sum(axis = 1).hist(ax = ax[1])
ax[1].ticklabel_format(useOffset=False, axis = 'x')
ax[1].set_title('Distribution of sum of ethnicity variables')

plt.tight_layout()
```


![png](output_51_0.png)


# Drop-column importance 
Disccused [here](http://explained.ai/rf-importance/index.html#5), the authors claim that drop-column should be the ground truth implementation for feature importance if we had sufficiently large computing resources. The way it works is: 
* compute baseline feature importance with permutation importance 
* drop a column, retrain, and then recompute feature importance scores
* The importance score for a column is the difference between the baseline and the score for the model missing that column

Now, the Malay-ness of the constituency and the proportion of above-90s become the most important features in our random forests model, and the Chinese-ness of the constituency becomes less important, but important nonetheless.  


```python
from rfpimp import dropcol_importances 
dc_importances = dropcol_importances(clf, X_train, Y_train)
dc_importances
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Importance</th>
    </tr>
    <tr>
      <th>Feature</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Orang Asli (%)</th>
      <td>0.030075</td>
    </tr>
    <tr>
      <th>Cina (%)</th>
      <td>0.015038</td>
    </tr>
    <tr>
      <th>Lain-Lain (%)</th>
      <td>0.015038</td>
    </tr>
    <tr>
      <th>India (%)</th>
      <td>0.007519</td>
    </tr>
    <tr>
      <th>Bumiputera Sabah (%)</th>
      <td>0.007519</td>
    </tr>
    <tr>
      <th>21 - 30 (%)</th>
      <td>-0.007519</td>
    </tr>
    <tr>
      <th>81 - 90 (%)</th>
      <td>-0.007519</td>
    </tr>
    <tr>
      <th>41 - 50 (%)</th>
      <td>-0.015038</td>
    </tr>
    <tr>
      <th>71 - 80 (%)</th>
      <td>-0.015038</td>
    </tr>
    <tr>
      <th>Bumiputera Sarawak (%)</th>
      <td>-0.015038</td>
    </tr>
    <tr>
      <th>31 - 40 (%)</th>
      <td>-0.022556</td>
    </tr>
    <tr>
      <th>51 - 60 (%)</th>
      <td>-0.022556</td>
    </tr>
    <tr>
      <th>61 - 70 (%)</th>
      <td>-0.022556</td>
    </tr>
    <tr>
      <th>Above 90 (%)</th>
      <td>-0.022556</td>
    </tr>
    <tr>
      <th>Melayu (%)</th>
      <td>-0.037594</td>
    </tr>
  </tbody>
</table>
</div>




```python
dc_importances.plot.barh()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a15e4b898>




![png](output_54_1.png)


Recall how the permutation score works - it seems that when including Melayu and all other negative-importance features, the model actually gets worse! Race seems to be the only thing that matters here. 

However, even this result isn't stable. It's probably just a function of the subsample that we get. I cheated and ran the random forest classifier and dropcol_importances on the entire dataset, but this time some of the age variables matter. 


```python
clf = RandomForestClassifier(n_estimators = 100, random_state = 3, oob_score = True)
clf.fit(ge14[features], ge14.target)

import rfpimp 
dc_importances = dropcol_importances(clf, ge14[features], ge14['target'])
dc_importances
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Importance</th>
    </tr>
    <tr>
      <th>Feature</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Cina (%)</th>
      <td>0.022523</td>
    </tr>
    <tr>
      <th>Orang Asli (%)</th>
      <td>0.018018</td>
    </tr>
    <tr>
      <th>21 - 30 (%)</th>
      <td>0.013514</td>
    </tr>
    <tr>
      <th>31 - 40 (%)</th>
      <td>0.013514</td>
    </tr>
    <tr>
      <th>51 - 60 (%)</th>
      <td>0.004505</td>
    </tr>
    <tr>
      <th>India (%)</th>
      <td>0.004505</td>
    </tr>
    <tr>
      <th>Bumiputera Sarawak (%)</th>
      <td>0.004505</td>
    </tr>
    <tr>
      <th>41 - 50 (%)</th>
      <td>-0.004505</td>
    </tr>
    <tr>
      <th>61 - 70 (%)</th>
      <td>-0.004505</td>
    </tr>
    <tr>
      <th>Melayu (%)</th>
      <td>-0.004505</td>
    </tr>
    <tr>
      <th>Lain-Lain (%)</th>
      <td>-0.004505</td>
    </tr>
    <tr>
      <th>Above 90 (%)</th>
      <td>-0.009009</td>
    </tr>
    <tr>
      <th>71 - 80 (%)</th>
      <td>-0.013514</td>
    </tr>
    <tr>
      <th>Bumiputera Sabah (%)</th>
      <td>-0.013514</td>
    </tr>
    <tr>
      <th>81 - 90 (%)</th>
      <td>-0.018018</td>
    </tr>
  </tbody>
</table>
</div>



# Meta-features: race matters a lot more than age
The author started out with the question of how race matters in comparison to age. It is possible to group correlated features into meta-features to calculate feature importance using the permutations approach: 


```python
from rfpimp import importances, plot_importances
```


```python
metafeatures = [['21 - 30 (%)', '31 - 40 (%)', '41 - 50 (%)', '51 - 60 (%)', '61 - 70 (%)','71 - 80 (%)', '81 - 90 (%)', 'Above 90 (%)'], 
        ['Melayu (%)', 'Cina (%)','India (%)', 'Bumiputera Sabah (%)', 'Bumiputera Sarawak (%)','Orang Asli (%)', 'Lain-Lain (%)']] 
I = importances(clf, X_test, Y_test, features=metafeatures) 
plot_importances(I)
```


![png](output_59_0.png)


I'm not sure if it's entirely kosher to do this, but for what it's worth, the ratio of the feature importance of ethnicity to age, calculated for 10 runs, is


```python
mylist = []
for i in range(10): 
    I = importances(clf, X_test, Y_test, features=metafeatures) 
    mylist.append(I.iloc[0,0]/I.iloc[1,0])
mylist
```




    [1.95,
     2.3333333333333335,
     2.6875,
     2.1875,
     2.176470588235294,
     2.2000000000000006,
     2.1250000000000004,
     2.3571428571428563,
     1.7368421052631582,
     2.4375]



# Conclusions: 
Using crude feature importances from scikit-learn, it's really unclear what's going on when each feature is the complement of the other. What does it mean if the proportion of Chinese is more important? Maybe having a constituency with more Chinese matters, but that may mean having a constituency with less Malays, and therefore having less Malays matters. But which matters? Trying to interpret each feature as it is, as in Research Question 2, is logically absurd. A meta-features approach using permutation importance is probably the more realistic approach here, or the other approach I can think of is to use the majority ethnicity as predictors. A second alternative, I guess, is to use clustering to classify different ethnic and age demographic compositions.

It is clear from this exercise that interpreting feature importance metrics should be done with caution. The feature importance numbers we have computed were unstable, probably due to the really small sample size, but also due to the strong collinearity between features. 

The way it's going, using the results of our random forest classifier as-is to measure feature importance provides unstable results. I would rather go with results from exploratory data analysis (EDA). 

I've done some supplementary work looking at differences in the importance of the age and ethnicity meta-features, as follows. Note that my results are unstable. 

## Peninsula



```python
X_train2, X_test2, Y_train2, Y_test2 = train_test_split(ge14[peninsula][features], 
                                                        ge14[peninsula]['target'],
                                                        test_size = 0.3, 
                                                        random_state = 42)

clf2 = RandomForestClassifier(n_estimators = 100, random_state = 3, oob_score = True)
clf2.fit(X_train2, Y_train2)
I2 = importances(clf2, X_test2, Y_test2, features=metafeatures) 
plot_importances(I2)
```


![png](output_63_0.png)



```python
mylist2 = []
for i in range(10): 
    I2 = importances(clf2, X_test2, Y_test2, features=metafeatures) 
    mylist2.append(I2.iloc[0,0]/I2.iloc[1,0])
mylist2
```




    [3.4999999999999973,
     2.9999999999999982,
     -10.00000000000005,
     6.499999999999995,
     1.9999999999999982,
     2.6666666666666647,
     8.999999999999995,
     -6.0000000000000275,
     2.3999999999999977,
     -5.0000000000000115]



## Borneo


```python
X_train3, X_test3, Y_train3, Y_test3 = train_test_split(ge14[np.invert(peninsula)][features], 
                                                        ge14[np.invert(peninsula)].loc[:,'target'],
                                                        test_size = 0.3, 
                                                        random_state = 42)

clf3 = RandomForestClassifier(n_estimators = 100, random_state = 3, oob_score = True)
clf3.fit(X_train3, Y_train3)
I3 = importances(clf3, X_test3, Y_test3, features=metafeatures) 
plot_importances(I3)
```


![png](output_66_0.png)



```python
mylist3 = []
for i in range(10): 
    I3 = importances(clf3, X_test3, Y_test3, features=metafeatures) 
    mylist3.append(I3.iloc[0,0]/I3.iloc[1,0])
mylist3
```

    /Users/philip/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in double_scalars
      after removing the cwd from sys.path.





    [inf,
     4.000000000000006,
     inf,
     3.000000000000004,
     4.000000000000006,
     3.000000000000004,
     3.000000000000004,
     3.000000000000004,
     inf,
     -2.9999999999999982]


